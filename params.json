{"name":"spaCy","tagline":"Lightning fast, full-cream NL tokenization for Python/Cython. Tokens are pointers to rich Lexeme structs.","body":"## If you're tokenizing English like it's a programming language, you're doing it all wrong\r\n\r\nI've been doing NLP research for almost ten years now, and I've hated every tokenizer I've ever used. Now that I've quit academia to work on my own projects and do some consulting, my need for a better solution has become more urgent. So, here's spaCy: a whitespace-based tokenizer, implemented in Cython. The idea in a nutshell:\r\n\r\nWe look-up whitespace-delimited chunks of text in a global vocabulary, and receive back a pointer to a linked-list node. We traverse the linked list to add all the tokens from the chunk, and return a vector of ints. To a Python client, the ints will be token IDs, but to our Cython internals, they're really points.\r\n\r\nLike this:\r\n\r\n    cpdef tokenize(unicode string):\r\n        start = -1\r\n        collect = False\r\n        vector[size_t] tokens = vector[size_t]()\r\n        for i in range(len(string)):\r\n            if is_whitespace(string[i]):\r\n                if start != -1:\r\n                    token = lookup(substr(string, start, i)))\r\n                    while token is not NULL:\r\n                        tokens.push_back(token)\r\n                        token = token.tail\r\n                    start = -1\r\n            elif start == -1:\r\n                start = i\r\n        return tokens\r\n\r\nTo see what's so good here, realize that almost all tokens already have a Lexeme object ready in the vocabulary --- only a small fraction of tokens represent new words. And if the word's in our vocabulary, all we're doing is hashing a substring, returning a pointer, and maybe traversing a linked list.\r\n\r\nThis approach is both much more efficient, and much more powerful. We can easily write tokenization rules to deal with all sorts of tricky cases, and we can tell every token all sorts of information about its lexical type, with almost no increase in memory usage.\r\n\r\nLet's look at some token properties we might want to ask about when writing feature functions.\r\n\r\nFor POS tagging, we often really care about capitalisation:\r\n\r\n    >>> from nltk.tokenize import word_tokenize\r\n    >>> string = \"Words are split on spaces, punctuation's split too.\"\r\n    >>> word_tokenize(string)\r\n    ['Words', 'are', 'split', 'on', 'spaces', ',', 'punctuation', \"'s\", 'split', 'too', '.']\r\n    >>> [word[0].isupper() for word in string]\r\n    [True, False…]\r\n    \r\nWe don't have the string anymore, but this property is type-constant --- so we can just store it. To the client, the API is only slightly different:\r\n\r\n    >>> from spacy import tokenize, is_upper\r\n    >>> string = \"Words are split on spaces, punctuation's split too.\"\r\n    >>> token_ids = tokenize(string)\r\n    >>> [is_upper(t) for t in token_ids]\r\n    [True, False…]\r\n    \r\nBut internally, we are more efficient:\r\n\r\n    cdef bint is_upper(size_t token_addr):\r\n        return (<Lexeme*>token_addr).is_upper\r\n\r\nOur approach is more powerful because we can cache the results of any function of a lexical type. The costs of the computation will be amortized over all of the individual occurrences of the word, so it's basically free.\r\n\r\nFor instance, the is_upper feature is trying to figure out whether the word is a named entity. So maybe what we want to do is load a frequencies file, pre-build from a huge sample, that asks how often \"words\" is spelled as \"Words\", how often it's spelled as \"WORDS\", and how often it's spelled as \"words\".\r\n\r\nWe then give the client a function oft_upper:\r\n\r\n    >>> from spacy import tokenize, oft_upper\r\n    >>> string = \"Words are split on spaces, punctuation's split too.\"\r\n    >>> token_ids = tokenize(string)\r\n    >>> [oft_title(t) for t in token_ids]\r\n    [False, False…]\r\n\r\nWhich we might implement as:\r\n\r\n    cdef bint oft_titlte(size_t token_addr):\r\n        return (<Lexeme*>token_addr).upper_pc >= 0.95\r\n        \r\ni.e., across a large sample of text, is the lower-cased version of that word spelled title-cased at least 95% of the time? Here's my current Lexeme struct:\r\n\r\n    cdef struct Lexeme:\r\n        uint64_t orig # Hash of the original string\r\n        uint64_t lex # Hash of the word, with extra stuff split off\r\n        uint64_t normed # Want to stem words, map dates to DATE, etc? Store the hash of that.\r\n        uint64_t cluster # Brown clusters are a useful distributional representation\r\n        Py_UNICODE first # First character of the token is often useful\r\n    \tuint64_t suffix # Last 3 characters of the token is often useful\r\n    \tbint oft_upper # Is the word often seen in all caps?\r\n    \tbint oft_title # Is the word often seen title-cased?\r\n    \tbint is_stop # Is the word in our stoplist?\r\n    \tbint can_noun # Do we ever see this word as a noun?\r\n    \tbint can_verb # ..A verb?\r\n    \tbint can_adj # ..An adjective?\r\n    \tbint can_adv # ..An adverb?\r\n\r\n\r\nOnce we do the work of consuming the lexical resource and adding it to our Lexeme representation, it'll be available for all of our applications from then on, at no extra client cost.\r\nBecause we only hold one Lexeme object per lexical _type_, not lexical token, we can make our Lexeme representation as rich as we want. Why not store the frequency of the word, estimated from a large sample of text? Whether the word can be noun? Its Brown cluster? Loading in this type-based knowledge is easy, and even web-scale text-processing tasks won't need a vocabulary of more than a few million lexical types, so we can fatten our lexical type objects freely.\r\n\r\nThe project, spaCy, is here on github. The rant, which you're reading, is below :).\r\n\r\nYour average tokenizer works like this:\r\n\r\n    >>> from nltk.tokenize import word_tokenize\r\n    >>> string = \"words are split on spaces, punctuation's split too.\"\r\n    >>> word_tokenize(string)\r\n    ['words', 'are', 'split', 'on', 'spaces', ',', 'punctuation', \"'s\", 'split', 'too', '.']\r\n    \r\nWe consume a string, and return a list of strings, one per token. This seems fine, but it actually really sucks, for almost any non-trivial application:\r\n\r\n1. Aligning the tokens back to string indices is difficult/impossible. Want to display text with mark-up? Have fun.\r\n2. The regex rules are hard to get right, and generally illegible. \r\n3. Naive implementations are terribly inefficient. \r\n4. For machine learning, we don't want strings anyway --- we want to compute with int vectors.\r\n\r\nMy solution is to tightly couple the tokenizer to a global vocabulary store. The tokenizer returns a vector of pointers/references to lexical types. In any normal sample of natural language, we'll have far fewer types than tokens. So, tokens are just references to types.\r\n\r\n\r\n\r\nOf these problems, only the first has really received much attention. The main proposal is to have the tokenizer return indexes into the string, which can then be used to fetch slices. I suppose that's okay if you didn't really plan to get anything much done, but for the rest of us it's hopelessly impractical.\r\n\r\nMy solution is to maintain a global vocabulary, and use it to map the string into a vector of integers. At first this seems inconvenient:\r\n\r\n    >>> from spacy.vocab import id_from_string, string_from_id\r\n    >>> word_id = id_from_string(\"words\")\r\n    88969807009L\r\n    >>> string_from_id(word_id)\r\n    \"words\"\r\n    \r\nSo far, this seems inconvenient. But now we're only ever computing on each lexical *type* once, instead of performing a bunch of work on every *token*. This means we can add all sorts of goodies to our Lexeme struct, which our word_id is secretly a pointer to: \r\n\r\n\r\n    \t\r\nWe don't have to worry about keeping these structs thin, because we only ever pass around const pointers to them --- the vocabulary maintains the sole reference to each lexeme. So even in truly enormous text processing tasks, we're not going to need more than a few hundred thousand Lexeme instances, so the vocabulary is unlikely to be our memory bottleneck.\r\n\r\nThe centralized vocabulary is also our secret weapon in writing a sane, lightning fast tokenizer. Here's the magic:\r\n\r\n    cdef struct Lexeme:\r\n        …\r\n        cdef Lexeme* tail\r\n        \r\nThe client need never know, but the Lexeme is actually a linked list. This means we can split the string *exclusively* on whitespace:\r\n\r\n    cpdef tokenize(unicode string):\r\n        start = -1\r\n        collect = False\r\n        tokens = vector[size_t]()\r\n        ws_positions = vector[size_t]()\r\n        ws_chars = vector[Py_UNICODE]()\r\n        for i in range(len(string)):\r\n            if is_whitespace(string[i]):\r\n                if start != -1:\r\n                    token = lookup(substr(string, start, i)))\r\n                    while token is not NULL:\r\n                        tokens.push_back(token)\r\n                        token = token.tail\r\n                    start = -1\r\n                ws_positions.push_back(i)\r\n                ws_chars.push_back(<Py_UNICODE>string[i])\r\n            elif start == -1:\r\n                start = i\r\n        return tokens\r\n\r\nThe idea here is that even when we stick on punctuation etc, we'll still see the same strings again and again, due to Zipf's law. So we cache the computation of \"I'm\" into \"I\", \"'m\" etc, by adding \"I'm\" to our vocabulary, and ensuring that we extend the token vector with tokens that have useful properties. Crucially, we want to get sensible results out of this:\r\n\r\n    >>> token_ids = tokenize(\"I'm a tricky string, aren't I?\")\r\n    >>> [lex_of(t) for t in token_ids]       \r\n    [\"I\", \"'m\", \"a\", \"tricky\", \"string\", \",\", \"are\" \"n't\", \"I\", \"?\"]\r\n\r\nThe tokenization is also fully reversible. All we have to do is store the inverse mapping, from token IDs to strings, use the whitespace characters and positions we stored when we tokenized, and count how many tokens to skip when we hit a token with a non-NULL tail. It's a bit fiddly, but it's a library function --- it never needs to be reimplemented.\r\n\r\nUnless we hit a word we've never seen before, \"lookup\" is just a hash table look up. And because the function that *does* hit new words is called so seldom, we're free to make it quite complicated, and deal with all sorts of tricky phenomena if we want. \r\n                ","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}