{"name":"spaCy","tagline":"Lightning fast, full-cream NL tokenization for Python/Cython. Tokens are pointers to rich Lexeme structs.","body":"## Why did you write this?\r\n\r\nI've been doing NLP research for about ten years now, and I've hated every tokenizer I've ever used. They're inefficient, brittle, and inexpressive. spaCy takes a very different approach.\r\n\r\n## What does it do?\r\n\r\nGiven a string of text, get a list of token IDs, and use them to access a variety of properties. You'll almost never need to do additional string processing:\r\n\r\n```python\r\n    >>> import spacy\r\n    >>> string = \"Features are calculated easily! And much more speedilyâ€¦\"\r\n    >>> word_ids = spacy.ids_from_string(string)\r\n```\r\nYou won't need to unhash like this often\r\n\r\n```python\r\n    >>> print(spacy.unhash(spacy.lex_of(word_ids[3])))\r\n    \"easily\"\r\n```\r\n\r\nCommon string-based features are pre-computed for you:\r\n\r\n```python\r\n    >>> spacy.last3_of(word_ids[3]) == spacy.last3_of(word_ids[8])\r\n    True\r\n```\r\n\r\nBut *DON'T* do this --- it'll be *almost always* correct, the WORST kind of incorrect:\r\n\r\n```python\r\n    >>> word_ids[0] == word_ids[1] # NO, WRONG WAY, GO BACK\r\n    (Answer too full of deceit and treachery to display)\r\n```\r\n## Why is this good?\r\n\r\nInstead of strings, you get a pointer to a set of all sorts of lexical type information. These are computed once per word in the vocabulary, so we can do sophisticated string normalization, or supply properties pre-computed from from large samples:\r\n\r\n```python\r\n    >>> import spacy\r\n    >>> from spacy.lex import prob_of, cluster, normed, best_lemma, is_oft_title, is_oft_upper\r\n    >>> Apples, grappled, oranges = space.ids_from_string(\"Apples grappled oranges\r\n    >>> # Load your own unigram probabilities, or use my Knesser-Ney smoothed estimates from Wikipedia.\r\n    >>> spacy.prob_of(Apples)\r\n    -30.6548\r\n    >>> # We store a bit-field of possible tags. Again, load your own data, or use good defaults.\r\n    >>> can_noun(Apples), can_verb(Apples), can_verb(grappled)\r\n    True False True\r\n    >>> # Integer of Brown cluster bit-string. Similar words receive better-than-chance similar values.\r\n    >>> (cluster_of(Apples) - cluster_of(grappled)) > cluster_of(oranges) - cluster_of(grappled)\r\n```\r\n\r\nspaCy returns you pointers to these rich lexical types over N times faster than a normal tokenizer can give you a list of strings:\r\n\r\nAnd, spaCy is 100% reversible/non-destructive: you can always recover indices into the original string, which is always difficult and sometimes impossible with standard tokenizers. Want to display text with inline mark-up? We have you covered. Other tokenizers? Not so much.\r\n\r\n## What is this sorcery? (Or, how does it work)\r\n\r\nThere are two things you need to know to understand how and why spaCy works:\r\n\r\n1. Zipf's law. The frequency distribution of word types is heavily skewed. The number of word tokens in a sample of text grows exponentially faster than the number of types. i.e., vocabularies are much smaller than texts.\r\n2. Cython, a language that lets us write code that seamlessly mixes low-level pointer semantics, and high level Python constructs. \r\n\r\nWhat we do is iterate through the characters in the text, and look up white-space delimited chunks in a global hash table. The table returns pointers to nodes in a linked-list, which we traverse to extend our vector of tokens. Like this:\r\n\r\n```cython\r\n    cpdef tokenize(unicode string):\r\n        start = -1\r\n        collect = False\r\n        vector[size_t] tokens = vector[size_t]()\r\n        for i in range(len(string)):\r\n            if is_whitespace(string[i]):\r\n                if start != -1:\r\n                    token = lookup(substr(string, start, i)))\r\n                    while token is not NULL:\r\n                        tokens.push_back(token)\r\n                        token = token.tail\r\n                    start = -1\r\n            elif start == -1:\r\n                start = i\r\n        return tokens\r\n```\r\n\r\nBecause of Zipf's law, almost every call to lookup is served by the hash-table. It's only when we see a previously unseen chunk that we must look within the string and create new Lexeme structs. By avoiding calls to that process, we're free to do more sophisticated processing, and use a simpler, more maintainable and reliable implementation than we would otherwise.\r\n\r\nA chunk is any sequence of non-whitespace characters, e.g. \"(Hello!)\" is a single chunk. When we encounter a previously-unseen chunk, we create a Lexeme for the first substring, \"(\", which knows its verbatim string (\"(Hello!)\". We then lookup the other substrings,  \"Hello!)\", \"!)\", and \")\", creating new Lexeme structs when we encounter one we haven't seen.\r\n\r\nBy maintaining a global vocabulary, each token is represented by a 64-bit integer, which we can easily use in numpy arrays, vectors, etc. But those 64-bit integers instantly decode to a rich representation, giving us everything we need to calculate features. And remember: our vocabulary of seen chunks is exponentially smaller than the number of tokens we have processed.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}