<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8' />
    <meta http-equiv="X-UA-Compatible" content="chrome=1" />
    <meta name="description" content="spaCy : Lightning fast, full-cream NL tokenization for Python/Cython. Tokens are pointers to rich Lexeme structs." />

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>spaCy</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/honnibal/spaCy">View on GitHub</a>

          <h1 id="project_title">spaCy</h1>
          <h2 id="project_tagline">Lightning fast, full-cream NL tokenization for Python/Cython. Tokens are pointers to rich Lexeme structs.</h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/honnibal/spaCy/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/honnibal/spaCy/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h2>
<a name="why-did-you-write-this" class="anchor" href="#why-did-you-write-this"><span class="octicon octicon-link"></span></a>Why did you write this?</h2>

<p>I've been doing NLP research for about ten years now, and I've hated every tokenizer I've ever used. They're inefficient, brittle, and inexpressive. spaCy takes a very different approach.</p>

<h2>
<a name="what-does-it-do" class="anchor" href="#what-does-it-do"><span class="octicon octicon-link"></span></a>What does it do?</h2>

<p>Given a string of text, get a list of token IDs, and use them to access a variety of properties. You'll almost never need to do additional string processing:</p>

<pre><code>&gt;&gt;&gt; import spacy
&gt;&gt;&gt; string = "Features are calculated easily! And much more speedilyâ€¦"
&gt;&gt;&gt; word_ids = spacy.ids_from_string(string)
&gt;&gt;&gt; # You won't need to unhash like this often
&gt;&gt;&gt; print(spacy.unhash(spacy.lex_of(word_ids[3])))
"easily"
&gt;&gt;&gt; # Instead, do this (check suffix match of easily/speedily)
&gt;&gt;&gt; spacy.last3_of(word_ids[3]) == spacy.last3_of(word_ids[8])
True
&gt;&gt;&gt; But _DON'T_ do this --- the raw ID ints have only internal significance
&gt;&gt;&gt; word_ids[0] == word_ids[1] # NO, WRONG WAY, GO BACK
(This answer may be full of lies! It'll do what you want about 95% of the time --- much worse than 0%)
</code></pre>

<h2>
<a name="why-is-it-good" class="anchor" href="#why-is-it-good"><span class="octicon octicon-link"></span></a>Why is it good?</h2>

<p>Instead of strings, you get a pointer to a set of all sorts of lexical type information. These are computed once per word in the vocabulary, so we can do sophisticated string normalization, or supply properties pre-computed from from large samples:</p>

<pre><code>&gt;&gt;&gt; import spacy
&gt;&gt;&gt; from spacy.lex import prob_of, cluster, normed, best_lemma, is_oft_title, is_oft_upper
&gt;&gt;&gt; Apples, grappled, oranges = space.ids_from_string("Apples grappled oranges
&gt;&gt;&gt; # Load your own unigram probabilities, or use my Knesser-Ney smoothed estimates from Wikipedia.
&gt;&gt;&gt; spacy.prob_of(Apples)
-30.6548
&gt;&gt;&gt; # We store a bit-field of possible tags. Again, load your own data, or use good defaults.
&gt;&gt;&gt; can_noun(Apples), can_verb(Apples), can_verb(grappled)
True False True
&gt;&gt;&gt; # Integer of Brown cluster bit-string. Similar words receive better-than-chance similar values.
&gt;&gt;&gt; (cluster_of(Apples) - cluster_of(grappled)) &gt; cluster_of(oranges) - cluster_of(grappled)
</code></pre>

<p>spaCy returns you pointers to these rich lexical types over N times faster than a normal tokenizer can give you a list of strings:</p>

<p>And, spaCy is 100% reversible/non-destructive: you can always recover indices into the original string, which is always difficult and sometimes impossible with standard tokenizers. Want to display text with inline mark-up? We have you covered. Other tokenizers? Not so much.</p>

<h2>
<a name="what-is-this-sorcery-or-how-does-it-work" class="anchor" href="#what-is-this-sorcery-or-how-does-it-work"><span class="octicon octicon-link"></span></a>What is this sorcery? (Or, how does it work)</h2>

<p>There are two things you need to know to understand how and why spaCy works:</p>

<ol>
<li>Zipf's law. The frequency distribution of word types is heavily skewed. The number of word tokens in a sample of text grows exponentially faster than the number of types. i.e., vocabularies are much smaller than texts.</li>
<li>Cython, a language that lets us write code that seamlessly mixes low-level pointer semantics, and high level Python constructs. </li>
</ol><p>What we do is iterate through the characters in the text, and look up white-space delimited chunks in a global hash table. The table returns pointers to nodes in a linked-list, which we traverse to extend our vector of tokens. Like this:</p>

<pre><code>cpdef tokenize(unicode string):
    start = -1
    collect = False
    vector[size_t] tokens = vector[size_t]()
    for i in range(len(string)):
        if is_whitespace(string[i]):
            if start != -1:
                token = lookup(substr(string, start, i)))
                while token is not NULL:
                    tokens.push_back(token)
                    token = token.tail
                start = -1
        elif start == -1:
            start = i
    return tokens
</code></pre>

<p>Because of Zipf's law, almost every call to lookup is served by the hash-table. It's only when we see a previously unseen chunk that we must look within the string and create new Lexeme structs. By avoiding calls to that process, we're free to do more sophisticated processing, and use a simpler, more maintainable and reliable implementation than we would otherwise.</p>

<p>A chunk is any sequence of non-whitespace characters, e.g. "(Hello!)" is a single chunk. When we encounter a previously-unseen chunk, we create a Lexeme for the first substring, "(", which knows its verbatim string ("(Hello!)". We then lookup the other substrings,  "Hello!)", "!)", and ")", creating new Lexeme structs when we encounter one we haven't seen.</p>

<p>By maintaining a global vocabulary, each token is represented by a 64-bit integer, which we can easily use in numpy arrays, vectors, etc. But those 64-bit integers instantly decode to a rich representation, giving us everything we need to calculate features. And remember: our vocabulary of seen chunks is exponentially smaller than the number of tokens we have processed.</p>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">spaCy maintained by <a href="https://github.com/honnibal">honnibal</a></p>
        <p>Published with <a href="http://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
